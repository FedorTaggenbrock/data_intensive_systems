{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/Spark_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JfE8lo75x4o",
        "outputId": "87dea60d-fa53-43c4-cf90-8542353c0472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=88254e5de68c20f44c87133595efab593f1092a6a36cb8facceed798b13145d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7upYcIjs5WGE"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import math\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark import RDD\n",
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EF0NwAu6B6q"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Practise\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1lNX5M45hje"
      },
      "outputs": [],
      "source": [
        "def kMeans(data: RDD, k: int, maxIterations: int) -> list:\n",
        "    # Initialize centroids randomly\n",
        "    centroids = [tuple(x) for x in data.takeSample(withReplacement=False, num=k)]\n",
        "\n",
        "    # Define a custom distance function\n",
        "    def distance(point1, point2):\n",
        "        return math.sqrt(sum([(x - y)**2 for x, y in zip(point1, point2)]))\n",
        "\n",
        "    # Iterate until convergence or until the maximum number of iterations is reached\n",
        "    for i in range(maxIterations):\n",
        "        # Assign each point to the closest centroid\n",
        "        clusters = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), point)).groupByKey()\n",
        "\n",
        "        # Compute new centroids as the mean of the points in each cluster\n",
        "        newCentroids = clusters.mapValues(lambda points: tuple([sum(x) / len(points) for x in zip(*points)])).collect()\n",
        "\n",
        "        # Update centroids\n",
        "        for oldCentroid, newCentroid in newCentroids:\n",
        "            index = centroids.index(oldCentroid)\n",
        "            centroids[index] = newCentroid\n",
        "\n",
        "    return [list(x) for x in centroids]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gUJhNWul-HNp"
      },
      "source": [
        "This code defines a kMeans function that takes an RDD of data points (represented as lists of floats), the number of clusters k, and the maximum number of iterations as input. The function initializes the centroids randomly and then iterates until convergence or until the maximum number of iterations is reached. In each iteration, each point is assigned to the closest centroid using a custom distance function (in this case, the Euclidean distance), and then new centroids are computed as the mean of the points in each cluster. Finally, the function returns the computed centroids.\n",
        "\n",
        "This is just one example of how you could implement K-Means clustering with a custom distance function in PySpark using map and reduce functions. You could adapt this code to use a different distance function or to implement other variations of the K-Means algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MXsVnSu-Iif",
        "outputId": "54c36557-6d7e-40f7-82ef-3f5277d1dba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.5, 0.5]\n",
            "[8.5, 8.5]\n"
          ]
        }
      ],
      "source": [
        "# Create an RDD of data points\n",
        "data = spark.sparkContext.parallelize([\n",
        "    [0.0, 0.0],\n",
        "    [1.0, 1.0],\n",
        "    [9.0, 8.0],\n",
        "    [8.0, 9.0]\n",
        "])\n",
        "\n",
        "# Cluster the data into two clusters\n",
        "centroids = kMeans(data, k=2, maxIterations=10)\n",
        "\n",
        "# Print the resulting centroids\n",
        "for centroid in centroids:\n",
        "    print(centroid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4tk-lwrFzf4"
      },
      "outputs": [],
      "source": [
        "print(\"test\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
