{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/Spark_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JfE8lo75x4o",
    "outputId": "87dea60d-fa53-43c4-cf90-8542353c0472"
   },
   "outputs": [],
   "source": [
    "# %pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7upYcIjs5WGE"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import math\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark import RDD\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2EF0NwAu6B6q"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practise\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9.0, 8.0), (1.0, 1.0), (0.0, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD of data points\n",
    "data = spark.sparkContext.parallelize([\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 1.0],\n",
    "    [9.0, 8.0],\n",
    "    [8.0, 12.0]\n",
    "])\n",
    "\n",
    "k = 3\n",
    "init_centroids = centroids = [tuple(x) for x in data.takeSample(withReplacement=False, num=k)]\n",
    "print(init_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "r1lNX5M45hje"
   },
   "outputs": [],
   "source": [
    "def kMeans(data: RDD, k: int, maxIterations: int, debug: bool) -> list:\n",
    "    # Initialize centroids randomly\n",
    "    centroids = [tuple(x) for x in data.takeSample(withReplacement=False, num=k)]\n",
    "\n",
    "    # Define a custom distance function\n",
    "    def distance(point1, point2):\n",
    "        return math.sqrt(sum([(x - y)**2 for x, y in zip(point1, point2)]))\n",
    "\n",
    "    # Iterate until convergence or until the maximum number of iterations is reached\n",
    "    for i in range(maxIterations):\n",
    "        # Assign each point to the closest centroid\n",
    "        clusters = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), point)).groupByKey()\n",
    "\n",
    "        # Compute new centroids as the mean of the points in each cluster\n",
    "        newCentroids = clusters.mapValues(lambda points: tuple([sum(x) / len(points) for x in zip(*points)])).collect()\n",
    "\n",
    "        # Update centroids\n",
    "        for oldCentroid, newCentroid in newCentroids:\n",
    "            index = centroids.index(oldCentroid)\n",
    "            centroids[index] = newCentroid\n",
    "        \n",
    "        if debug:\n",
    "            # Calculate and print for each datapoint which cluster it belongs to\n",
    "            clusters = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), point)).groupByKey().collect()\n",
    "\n",
    "            # print(clusters)\n",
    "            # break\n",
    "        \n",
    "            # Print which iter\n",
    "            print(\"Iteration {}\".format(i))\n",
    "            for centroid, points in clusters:\n",
    "                print(\"Cluster with centroid {}: {}\".format(centroid, list(points)))\n",
    "            print(\"-------------------------------------------\")\n",
    "\n",
    "\n",
    "    return [list(x) for x in centroids]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gUJhNWul-HNp"
   },
   "source": [
    "This code defines a kMeans function that takes an RDD of data points (represented as lists of floats), the number of clusters k, and the maximum number of iterations as input. The function initializes the centroids randomly and then iterates until convergence or until the maximum number of iterations is reached. In each iteration, each point is assigned to the closest centroid using a custom distance function (in this case, the Euclidean distance), and then new centroids are computed as the mean of the points in each cluster. Finally, the function returns the computed centroids.\n",
    "\n",
    "This is just one example of how you could implement K-Means clustering with a custom distance function in PySpark using map and reduce functions. You could adapt this code to use a different distance function or to implement other variations of the K-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Cluster with centroid (8.5, 10.0): [[9.0, 8.0], [8.0, 12.0]]\n",
      "Cluster with centroid (0.5, 0.5): [[0.0, 0.0], [1.0, 1.0]]\n",
      "-------------------------------------------\n",
      "Iteration 1\n",
      "Cluster with centroid (8.5, 10.0): [[9.0, 8.0], [8.0, 12.0]]\n",
      "Cluster with centroid (0.5, 0.5): [[0.0, 0.0], [1.0, 1.0]]\n",
      "-------------------------------------------\n",
      "Iteration 2\n",
      "Cluster with centroid (8.5, 10.0): [[9.0, 8.0], [8.0, 12.0]]\n",
      "Cluster with centroid (0.5, 0.5): [[0.0, 0.0], [1.0, 1.0]]\n",
      "-------------------------------------------\n",
      "Iteration 3\n",
      "Cluster with centroid (8.5, 10.0): [[9.0, 8.0], [8.0, 12.0]]\n",
      "Cluster with centroid (0.5, 0.5): [[0.0, 0.0], [1.0, 1.0]]\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cluster the data into two clusters\n",
    "centroids = kMeans(data, k=2, maxIterations=2, debug=True)\n",
    "\n",
    "# Print the resulting centroids\n",
    "for centroid in centroids:\n",
    "    print(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4tk-lwrFzf4"
   },
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjyqVvws25Wh"
   },
   "outputs": [],
   "source": [
    "print(\"test3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
