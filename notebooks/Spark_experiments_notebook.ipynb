{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/Spark_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JfE8lo75x4o",
        "outputId": "1dd49d20-ee63-4042-be1f-2471a745f40d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=9bdecaa438ac985b6011d19b02f3d7323f24439f79e6bddf729142eb42ba6885\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dN6bLOo024eB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7upYcIjs5WGE"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import math\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark import RDD\n",
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2EF0NwAu6B6q"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Practise\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "r1lNX5M45hje"
      },
      "outputs": [],
      "source": [
        "def kMeans(distance, data: RDD, k: int, maxIterations: int) -> list:\n",
        "    # Initialize centroids randomly\n",
        "    centroids = [tuple(x) for x in data.takeSample(withReplacement=False, num=k)]\n",
        "\n",
        "    # Iterate until convergence or until the maximum number of iterations is reached\n",
        "    for i in range(maxIterations):\n",
        "        # Assign each point to the closest centroid\n",
        "        clusters = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), point)).groupByKey()\n",
        "\n",
        "        # Compute new centroids as the mean of the points in each cluster\n",
        "        newCentroids = clusters.mapValues(lambda points: tuple([sum(x) / len(points) for x in zip(*points)])).collect()\n",
        "\n",
        "        # Update centroids\n",
        "        for oldCentroid, newCentroid in newCentroids:\n",
        "            index = centroids.index(oldCentroid)\n",
        "            centroids[index] = newCentroid\n",
        "\n",
        "    return [list(x) for x in centroids]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUJhNWul-HNp"
      },
      "source": [
        "This code defines a kMeans function that takes an RDD of data points (represented as lists of floats), the number of clusters k, and the maximum number of iterations as input. The function initializes the centroids randomly and then iterates until convergence or until the maximum number of iterations is reached. In each iteration, each point is assigned to the closest centroid using a custom distance function (in this case, the Euclidean distance), and then new centroids are computed as the mean of the points in each cluster. Finally, the function returns the computed centroids.\n",
        "\n",
        "This is just one example of how you could implement K-Means clustering with a custom distance function in PySpark using map and reduce functions. You could adapt this code to use a different distance function or to implement other variations of the K-Means algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "f4tk-lwrFzf4",
        "outputId": "b6bad2e0-c758-44a6-b5b6-c6ab7d07c00a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((0, 0), (1.0, 1.0)), ((10, 10), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[((1.0, 1.0), (1.0, 1.0)), ((11.0, 11.0), (11.0, 11.0))]\n",
            "[11.0, 11.0]\n",
            "[1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "#TESTING K-MEANS\n",
        "\n",
        "data2 = spark.sparkContext.parallelize([\n",
        "    [0, 0],\n",
        "    [10,10],\n",
        "    [2,2],\n",
        "    [12,12]\n",
        "])\n",
        "\n",
        "# Define a custom distance function\n",
        "def eaclid_distance(point1, point2):\n",
        "    return math.sqrt(sum([(x - y)**2 for x, y in zip(point1, point2)]))\n",
        "# Cluster the data into two clusters\n",
        "centroids = kMeans(eaclid_distance, data2, k=2, maxIterations=10)\n",
        "\n",
        "# Print the resulting centroids\n",
        "for centroid in centroids:\n",
        "    print(centroid)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from collections import Counter\n",
        "\n",
        "def kModes_v1(distance, data: RDD, k: int, maxIterations: int, list_size: int) -> list:\n",
        "    # Initialize centroids randomly\n",
        "    centroids = [tuple(x) for x in data.takeSample(withReplacement=False, num=k)]\n",
        "\n",
        "    # Iterate until convergence or until the maximum number of iterations is reached\n",
        "    for i in range(maxIterations):\n",
        "        print(\"centroids = \", centroids)\n",
        "\n",
        "        # Assign each point to the closest centroid\n",
        "        clusters = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), point))\n",
        "\n",
        "        #print(\"clusters1 = \", clusters.collect())\n",
        "\n",
        "        #Compute new centroids as the mode of the points in each cluster\n",
        "        clusters = clusters.mapValues(lambda set: Counter(set))\n",
        "        clusters = clusters.reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "        #print(\"clusters2 = \", clusters.collect())\n",
        "\n",
        "        newCentroids = clusters.mapValues(lambda counter:tuple([c[0] for c in counter.most_common(4)])).collect()\n",
        "\n",
        "        #print(\"newCentroids = \", newCentroids)\n",
        "\n",
        "        # Update centroids\n",
        "        for oldCentroid, newCentroid in newCentroids:\n",
        "            index = centroids.index(oldCentroid)\n",
        "            centroids[index] = newCentroid\n",
        "\n",
        "    return [list(x) for x in centroids]"
      ],
      "metadata": {
        "id": "FIkM6waCOrvP"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING K-MODES USING CATEGORICAL DATA\n",
        "\n",
        "#data is encoded zodat elke kolom een 1 of een 0 bevat afhankelijk van of die trip gedaan wordt.\n",
        "data = spark.sparkContext.parallelize([\n",
        "    [\"A\", \"B\", \"C\", \"K\"],\n",
        "    [\"F\", \"E\", \"D\", \"B\"],\n",
        "    [\"B\", \"C\", \"A\",\"J\"],\n",
        "    [\"D\", \"E\", \"A\", \"F\"],\n",
        "])\n",
        "# Define a custom distance function\n",
        "def jaccard_distance(a, b):\n",
        "    a = set(a)\n",
        "    b = set(b)\n",
        "    intersection = len(a & b)\n",
        "    union = len(a | b)\n",
        "    return 1 - (intersection / union)\n",
        "\n",
        "# Cluster the data into two clusters\n",
        "centroids = kModes_v1(jaccard_distance, data, k=2, maxIterations=10, list_size=4)\n",
        "\n",
        "# Print the resulting centroids\n",
        "for centroid in centroids:\n",
        "    print(centroid)\n"
      ],
      "metadata": {
        "id": "lEnSNbNiOfb7",
        "outputId": "af304b6c-9314-47b5-8a94-e11981f25563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centroids =  [('A', 'B', 'C', 'K'), ('B', 'C', 'A', 'J')]\n",
            "centroids =  [('A', 'B', 'F', 'E'), ('B', 'C', 'A', 'J')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "centroids =  [('F', 'E', 'D', 'B'), ('A', 'B', 'C', 'K')]\n",
            "['F', 'E', 'D', 'B']\n",
            "['A', 'B', 'C', 'K']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import numpy as np\n",
        "from statistics import mode\n",
        "\n",
        "def kModes_v2(distance, data: RDD, k: int, maxIterations: int, list_size: int) -> list:\n",
        "    # Initialize centroids randomly\n",
        "    centroids = [tuple(x) for x in data.takeSample(withReplacement=False, num=k)]\n",
        "\n",
        "    # Iterate until convergence or until the maximum number of iterations is reached\n",
        "    for i in range(maxIterations):\n",
        "        print(\"centroids = \", centroids)\n",
        "\n",
        "        # Assign each point to the closest centroid\n",
        "        clusters = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), point)).groupByKey()\n",
        "\n",
        "        #print(\"clusters1 = \", clusters.collect())\n",
        "\n",
        "        #Compute new centroids as the mode of the points in each cluster\n",
        "        newCentroids = clusters.mapValues(lambda arrays: tuple([mode(x) for x in zip(*arrays)]) ).collect()\n",
        "\n",
        "        #print(\"newCentroids = \", newCentroids)\n",
        "\n",
        "        # Update centroids\n",
        "        for oldCentroid, newCentroid in newCentroids:\n",
        "            index = centroids.index(oldCentroid)\n",
        "            centroids[index] = newCentroid\n",
        "\n",
        "    return [list(x) for x in centroids]"
      ],
      "metadata": {
        "id": "a9XBPX3TqgjB"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "#Dit is waarschijnlijk de beste manier om het te doen, gebruikt encoding. Elke colom representeerd een bepaalde trip, bv utrecht-> ams en een route heeft een 1 voor die kolom als deze trip in de route zit. \n",
        "\n",
        "data = spark.sparkContext.parallelize([\n",
        "    [1,1,0,1,0],\n",
        "    [1,1,1,1,0],\n",
        "    [0,0,1,0,1],\n",
        "    [1,0,0,0,1],\n",
        "    [1,0,0,1,0],\n",
        "    [1,1,1,1,0],\n",
        "    [0,1,1,0,1],\n",
        "    [1,0,0,1,0],\n",
        "])\n",
        "\n",
        "# Cluster the data into two clusters using the k-modes algorithm with a custom distance function. \n",
        "centroids = kModes_v2(scipy.spatial.distance.jaccard, data, k=2, maxIterations=10, list_size = 5)\n",
        "\n",
        "# Print the resulting centroids\n",
        "for centroid in centroids:\n",
        "    print(centroid)"
      ],
      "metadata": {
        "id": "wB-7hxrlplOS",
        "outputId": "84f3fa69-dcff-4ab0-9019-663f1c8a1dce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 1, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "centroids =  [(0, 0, 1, 0, 1), (1, 1, 0, 1, 0)]\n",
            "[0, 0, 1, 0, 1]\n",
            "[1, 1, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p9MhtBhYq16j"
      },
      "execution_count": 228,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}