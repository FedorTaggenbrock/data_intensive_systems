{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/main_tests_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aPsU73HSlLI"
   },
   "source": [
    "**Handle importing/installing, both local and on Colab** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INJqht4-StzH",
    "outputId": "f4b73af0-7103-48cd-ccaa-a0c84fafe637"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m310.8/310.8 MB\u001B[0m \u001B[31m5.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=ad54a06f9ac86ec086c74fc66e1cfa693b754fc26b4e6f76e6c198bea6bd44b8\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "if ON_COLAB:\n",
    "    # Do stuff that only needs to happen on colab\n",
    "    !pip install pyspark  # noqa\n",
    "    pass\n",
    "else:\n",
    "    # Do stuff that only needs to happen on local computer\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rerun the code cell below to use the latest version of the python files!"
   ],
   "metadata": {
    "collapsed": false,
    "id": "b6jttgVIJsPe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_cMlVTtz4Ad",
    "outputId": "a1df6f33-112a-45a0-cf4f-2e8f6588ac2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running in Colab: importing from GitHub.\n",
      "fatal: destination path 'data_intensive_systems' already exists and is not an empty directory.\n",
      "/content/data_intensive_systems\n",
      "remote: Enumerating objects: 13, done.\u001B[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001B[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001B[K\n",
      "remote: Total 7 (delta 3), reused 7 (delta 3), pack-reused 0\u001B[K\n",
      "Unpacking objects: 100% (7/7), 1.90 KiB | 974.00 KiB/s, done.\n",
      "From https://github.com/FedorTaggenbrock/data_intensive_systems\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   4f4fb24..05ebb6b  main       -> origin/main\n",
      "Updating 4f4fb24..05ebb6b\n",
      "Fast-forward\n",
      " src/__pycache__/clustering.cpython-39.pyc | Bin \u001B[31m4454\u001B[m -> \u001B[32m3981\u001B[m bytes\n",
      " src/clustering.py                         |  37 \u001B[31m------------------------\u001B[m\n",
      " src/main_tests.py                         |  46 \u001B[32m++++++++++++++++++++++++++++++\u001B[m\n",
      " 3 files changed, 46 insertions(+), 37 deletions(-)\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "def import_modules():\n",
    "    if ON_COLAB:\n",
    "        print(\"Running in Colab: importing from GitHub.\")\n",
    "        # Clone the repository\n",
    "        !git clone https://github.com/FedorTaggenbrock/data_intensive_systems.git\n",
    "\n",
    "        # Change the current working directory to the cloned repository directory\n",
    "        %cd /content/data_intensive_systems\n",
    "\n",
    "        # Pull the latest changes from the repository\n",
    "        !git pull origin main\n",
    "\n",
    "        # Change back to the original working directory\n",
    "        %cd /content\n",
    "\n",
    "        # Add the path of the modules to sys.path\n",
    "        sys.path.insert(0, \"/content/data_intensive_systems/src\")\n",
    "    else:\n",
    "        print(\"Running locally, importing from local.\")\n",
    "        # Append parent folder to path\n",
    "        sys.path.append('../src')\n",
    "\n",
    "    # Import and reload modules iteratively\n",
    "    module_names = ['clustering', 'evaluate_clustering', 'generate_data',\n",
    "                    'parse_data', 'main_tests']\n",
    "    for module_name in module_names:\n",
    "        module = importlib.import_module(module_name)\n",
    "        importlib.reload(module)\n",
    "        globals()[module_name] = module\n",
    "\n",
    "import_modules()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEkh8sL6N742",
    "outputId": "b6c376eb-e947-465a-9d1b-4c68f1184e23"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initialized Spark.\n",
      "Running run_clustering().\n",
      "The centroids are given by:  [([[1, 1, 0, 1, 0], [0, 0, 1, 0, 1]], {'k': 2}), ([[0, 0, 1, 0, 1], [1, 1, 1, 1, 0], [1, 0, 0, 1, 0]], {'k': 3})]\n",
      "Start evaluating clusters\n"
     ]
    }
   ],
   "source": [
    "main_tests.run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test all functions you want inside the run_all_tests() during development, for small sample sizes.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "FMg9vvjcJsPg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code below is for actual result generation later, so that we can easily reuse intermediate values."
   ],
   "metadata": {
    "collapsed": false,
    "id": "BRrS6t9JJsPg"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoGwAnHNN743"
   },
   "source": [
    "**Create a spark instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MN8NRmi1N743",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "outputId": "806faa7d-0417-4077-bf75-2d5979a24d2e"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-e48a53e48df8>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mspark_settings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mspark_instance\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmake_spark\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'make_spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark_settings = {}\n",
    "spark_instance = make_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeNSBK7WN744"
   },
   "source": [
    "**Load and parse data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wg4AJxZRN744"
   },
   "outputs": [],
   "source": [
    "# Load and parse data\n",
    "data_for_spark = parse_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7RmOlOp7Ujp"
   },
   "source": [
    "**Perform clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqUifQ0kN745"
   },
   "outputs": [],
   "source": [
    "# Run clustering and parameter tuning\n",
    "clustering_settings = {\n",
    "    'clustering_algorithm':'kmodes',\n",
    "}\n",
    "clustering_centroid_outcomes = run_clustering(\n",
    "    spark_instance=spark_instance,\n",
    "    data_for_spark, clustering_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06CxnhSWN745"
   },
   "outputs": [],
   "source": [
    "# Evaluate clustering results\n",
    "clustering_evaluation = evaluate_clustering(clustering_centroid_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrrnSkhZ7YhR"
   },
   "source": [
    "**Display results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWgrghsBN746"
   },
   "outputs": [],
   "source": [
    "# Display results (maybe only best result)\n",
    "clustering_result = display_results(clustering_evaluation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
