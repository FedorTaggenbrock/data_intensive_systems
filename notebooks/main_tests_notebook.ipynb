{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/notebooks/main_tests_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1m1UZvaN74z",
        "outputId": "6e57c1d0-fd72-4b3c-d553-709ee7298279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/../main_tests.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# !python ../main_tests.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcdvOolnPS8O"
      },
      "outputs": [],
      "source": [
        "# if ON_COLAB:\n",
        "#     print(\"Running in Colab: importing from GitHub.\")\n",
        "#     # wget (\"curl\") all files from github\n",
        "#     user = 'FedorTaggenbrock'\n",
        "#     repo = 'data_intensive_systems'\n",
        "#     src_dir = \"src\"\n",
        "#     pyfiles = [\"clustering.py\"]\n",
        "\n",
        "#     for pyfile in pyfiles:\n",
        "#         url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{pyfile}\"\n",
        "#         !wget --no-cache --backups=1 {url}\n",
        "\n",
        "#     # Import all files\n",
        "#     try:\n",
        "#         import clustering as clustering\n",
        "#     except Exception as e:\n",
        "#         print('Error importing files on colab. Are the names right?.')\n",
        "\n",
        "# else:\n",
        "#     print(\"Running locally, importing from local.\")\n",
        "#     # Append parent folder to path\n",
        "#     sys.path.append('..')\n",
        "#     # And import all files\n",
        "#     from src import clustering as clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esFX-UWVv6QV",
        "outputId": "b0424979-db9d-4400-ffbe-e8ffe0aa3c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "delete /content/main_tests.py\n",
            "0.25\n",
            "update 14:40\n",
            "Initialized Spark. Start clustering.\n",
            "centroids =  [(1, 1, 1, 1, 0), (0, 0, 1, 0, 1)]\n",
            "centroids =  [(1, 1, 0, 1, 0), (0, 0, 1, 0, 1)]\n",
            "Finished clustering. Start evaluation.\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import requests\n",
        "# import sys\n",
        "# import time\n",
        "# import importlib\n",
        "\n",
        "# def get_file_from_github(raw_url, target_file):\n",
        "#     # Add a unique query parameter to the URL to bypass the cache\n",
        "#     raw_url += '?t=' + str(time.time())\n",
        "#     response = requests.get(raw_url)\n",
        "#     response.raise_for_status()  # Ensure we got a valid response\n",
        "#     with open(target_file, 'wb') as out_file:\n",
        "#         out_file.write(response.content)\n",
        "\n",
        "# def import_from_github(raw_urls, module_names):\n",
        "#     # Set your target path here\n",
        "#     # In Google Colab, you can use the current directory\n",
        "#     target_path = os.getcwd()\n",
        "\n",
        "#     modules = []\n",
        "#     # Download files\n",
        "#     for raw_url, module_name in zip(raw_urls, module_names):\n",
        "#         target_file = f\"{target_path}/{module_name}.py\"\n",
        "\n",
        "#         # If the module file exists, delete it first\n",
        "#         if os.path.exists(target_file):\n",
        "#             print(f'delete {target_file}')\n",
        "#             os.remove(target_file)\n",
        "\n",
        "#         get_file_from_github(raw_url, target_file)\n",
        "\n",
        "#         # If the module has been imported before, reload it\n",
        "#         if module_name in sys.modules:\n",
        "#             modules.append(importlib.reload(sys.modules[module_name]))\n",
        "#         else:\n",
        "#             # Add the directory to sys.path\n",
        "#             sys.path.insert(0, target_path)\n",
        "#             # Import your module\n",
        "#             modules.append(__import__(module_name))\n",
        "\n",
        "#     return modules\n",
        "\n",
        "\n",
        "# user = 'FedorTaggenbrock'\n",
        "# repo = 'data_intensive_systems'\n",
        "# src_dir = \"src\"\n",
        "# module_names = [\"clustering\", 'main_tests']\n",
        "# raw_urls = [f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{modname}.py\" for modname in module_names]\n",
        "\n",
        "# clustering, main_tests = import_from_github(raw_urls, module_names)\n",
        "\n",
        "# # Now you can use your modules\n",
        "# res = clustering.jaccard_distance([1,1,0,1,0], [1,1,1,1,0])\n",
        "# print(res)\n",
        "# clustering.clustering_test1()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPsU73HSlLI"
      },
      "source": [
        "**Handle importing/installing, both local and on Colab** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INJqht4-StzH",
        "outputId": "feabeba9-64df-46f4-bf3b-9d2d0535b9f5"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "ON_COLAB = 'google.colab' in sys.modules\n",
        "if ON_COLAB:\n",
        "    # Do stuff that only needs to happen on colab\n",
        "    !pip install pyspark  # noqa\n",
        "    pass\n",
        "else:\n",
        "    # Do stuff that only needs to happen on local computer\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ignore any errors raised by the following line\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_cMlVTtz4Ad",
        "outputId": "6573a31f-96c4-4222-de13-1979bb5c563c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally, importing from local.\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "def import_modules():\n",
        "    if ON_COLAB:\n",
        "        print(\"Running in Colab: importing from GitHub.\")\n",
        "        # Clone the repository\n",
        "        !git clone https://github.com/FedorTaggenbrock/data_intensive_systems.git\n",
        "\n",
        "        # Change the current working directory to the cloned repository directory\n",
        "        %cd /content/data_intensive_systems\n",
        "\n",
        "        # Pull the latest changes from the repository\n",
        "        !git pull origin main\n",
        "\n",
        "        # Change back to the original working directory\n",
        "        %cd /content\n",
        "\n",
        "        # Add the path of the modules to sys.path\n",
        "        sys.path.insert(0, \"/content/data_intensive_systems/src\")\n",
        "    else:\n",
        "        print(\"Running locally, importing from local.\")\n",
        "        # Append parent folder to path\n",
        "        sys.path.append('..')\n",
        "\n",
        "    # Import and reload modules iteratively\n",
        "    module_names = ['clustering', 'evaluate_clustering', 'generate_data',\n",
        "                    'parse_data', 'main_tests']\n",
        "    for module_name in module_names:\n",
        "        module = importlib.import_module(module_name)\n",
        "        importlib.reload(module)\n",
        "        globals()[module_name] = module\n",
        "\n",
        "import_modules()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxgQD6vN741"
      },
      "source": [
        "**Test if this works**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEkh8sL6N742",
        "outputId": "0b7357a1-3f7f-440a-f485-59d4c9e4d9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25\n",
            "update 15:21\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@DESKTOP-G15HP00.domain_not_set.invalid:53890\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:250)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:590)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m res \u001b[39m=\u001b[39m clustering\u001b[39m.\u001b[39mjaccard_distance([\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m], [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(res)\n\u001b[1;32m----> 4\u001b[0m clustering\u001b[39m.\u001b[39;49mclustering_test1()\n",
            "File \u001b[1;32mc:\\Users\\T460\\Documents\\Uni_spul\\Jaar_6\\DIS\\Project\\data_intensive_systems\\src\\clustering.py:122\u001b[0m, in \u001b[0;36mclustering_test1\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclustering_test1\u001b[39m():\n\u001b[0;32m    120\u001b[0m     \u001b[39m# Testing code\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mupdate 15:21\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m     spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mClustering\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n\u001b[0;32m    124\u001b[0m     data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39mparallelize([\n\u001b[0;32m    125\u001b[0m             [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],\n\u001b[0;32m    126\u001b[0m             [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m             [\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m],\n\u001b[0;32m    133\u001b[0m         ])\n\u001b[0;32m    135\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitialized Spark. Start clustering.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    198\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[0;32m    203\u001b[0m         sparkHome,\n\u001b[0;32m    204\u001b[0m         pyFiles,\n\u001b[0;32m    205\u001b[0m         environment,\n\u001b[0;32m    206\u001b[0m         batchSize,\n\u001b[0;32m    207\u001b[0m         serializer,\n\u001b[0;32m    208\u001b[0m         conf,\n\u001b[0;32m    209\u001b[0m         jsc,\n\u001b[0;32m    210\u001b[0m         profiler_cls,\n\u001b[0;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    286\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[0;32m    288\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@DESKTOP-G15HP00.domain_not_set.invalid:53890\r\n\tat org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)\r\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:140)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\r\n\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\r\n\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:250)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:235)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:590)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
          ]
        }
      ],
      "source": [
        "# print(dir(clustering))\n",
        "res = clustering.jaccard_distance([1,1,0,1,0], [1,1,1,1,0])\n",
        "print(res)\n",
        "clustering.clustering_test1()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qoGwAnHNN743"
      },
      "source": [
        "**Create a spark instance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8NRmi1N743"
      },
      "outputs": [],
      "source": [
        "spark_settings = {}\n",
        "spark_instance = make_spark()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FeNSBK7WN744"
      },
      "source": [
        "**Load and parse data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg4AJxZRN744"
      },
      "outputs": [],
      "source": [
        "# Load and parse data\n",
        "data_for_spark = parse_data()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z7RmOlOp7Ujp"
      },
      "source": [
        "**Perform clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqUifQ0kN745"
      },
      "outputs": [],
      "source": [
        "# Run clustering and parameter tuning\n",
        "clustering_settings = {}\n",
        "clustering_centroid_outcomes = run_clustering(spark_instance, data_for_spark, clustering_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CxnhSWN745"
      },
      "outputs": [],
      "source": [
        "# Evaluate clustering results\n",
        "clustering_evaluation = evaluate_clustering(clustering_centroid_outcomes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WrrnSkhZ7YhR"
      },
      "source": [
        "**Display results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWgrghsBN746"
      },
      "outputs": [],
      "source": [
        "# Display results (maybe only best result)\n",
        "clustering_result = display_results(clustering_evaluation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
