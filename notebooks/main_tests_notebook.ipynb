{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/notebooks/main_tests_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1m1UZvaN74z",
        "outputId": "5943fb0b-8c6e-4c61-c917-3a1340371c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test\n"
          ]
        }
      ],
      "source": [
        "!python ../main_tests.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPsU73HSlLI"
      },
      "source": [
        "**Pip install stuff that needs to be installed before importing anything**  \n",
        "(for colab only; cannot be done inside the modules, would be stupid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INJqht4-StzH",
        "outputId": "f3cdc703-54eb-41fa-b36d-83bfd9c7b2cd"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "ON_COLAB = 'google.colab' in sys.modules\n",
        "if ON_COLAB:\n",
        "    # Do stuff that only needs to happen on colab\n",
        "    !pip install pyspark\n",
        "else:\n",
        "    # Do stuff that only needs to happen on local computer\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcdvOolnPS8O",
        "outputId": "298aee47-0bf0-4ea8-90ce-7e7eb9a63d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running locally, importing from local.\n"
          ]
        }
      ],
      "source": [
        "if ON_COLAB:\n",
        "    print(\"Running in Colab: importing from GitHub.\")\n",
        "    # wget (\"curl\") all files from github\n",
        "    user = 'FedorTaggenbrock'\n",
        "    repo = 'data_intensive_systems'\n",
        "    src_dir = \"src\"\n",
        "    pyfiles = [\"clustering.py\"]\n",
        "\n",
        "    for pyfile in pyfiles:\n",
        "        url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{pyfile}\"\n",
        "        !wget --no-cache --backups=1 {url}\n",
        "\n",
        "    # Import all files\n",
        "    try:\n",
        "        import clustering as clustering\n",
        "    except Exception as e:\n",
        "        print('Error importing files on colab. Are the names right?.')\n",
        "\n",
        "else:\n",
        "    print(\"Running locally, importing from local.\")\n",
        "    # Append parent folder to path\n",
        "    sys.path.append('..')\n",
        "    # And import all files\n",
        "    from src import clustering as clustering\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxgQD6vN741"
      },
      "source": [
        "**Test if this works**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEkh8sL6N742",
        "outputId": "74749b8f-4367-4e45-96d8-73895b3ac8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25\n",
            "Initialized Spark. Start clustering.\n",
            "centroids =  [(1, 1, 0, 1, 0), (1, 0, 0, 0, 1)]\n",
            "centroids =  [(1, 1, 0, 1, 0), (0, 0, 1, 0, 1)]\n",
            "Finished clustering. Start evaluation.\n",
            "[1, 1, 0, 1, 0]\n",
            "[0, 0, 1, 0, 1]\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 25) (145.136.150.11 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\nTypeError: 'module' object is not callable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\nTypeError: 'module' object is not callable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m res \u001b[39m=\u001b[39m clustering\u001b[39m.\u001b[39mjaccard_distance([\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m], [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(res)\n\u001b[1;32m----> 3\u001b[0m clustering\u001b[39m.\u001b[39;49mclustering_test1()\n",
            "File \u001b[1;32mc:\\Users\\T460\\Documents\\Uni_spul\\Jaar_6\\DIS\\Project\\data_intensive_systems\\src\\clustering.py:145\u001b[0m, in \u001b[0;36mclustering_test1\u001b[1;34m()\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[39mprint\u001b[39m(centroid)\n\u001b[0;32m    144\u001b[0m \u001b[39m# Print the evaluation metrics\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[39mprint\u001b[39m(evaluate_clustering(data, centroids, clustering_setting\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mkModes\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
            "File \u001b[1;32mc:\\Users\\T460\\Documents\\Uni_spul\\Jaar_6\\DIS\\Project\\data_intensive_systems\\src\\clustering.py:78\u001b[0m, in \u001b[0;36mevaluate_clustering\u001b[1;34m(data, centroids, clustering_setting, perfect_centroids)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mEvaluate the clustering of the given data using the given centroids and clustering setting.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39m    NotImplementedError: If the clustering setting is recognized but not implemented.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m clustering_setting \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkModes\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     evaluation_metrics \u001b[39m=\u001b[39m evaluate_kModes(data, centroids, perfect_centroids\u001b[39m=\u001b[39;49mperfect_centroids)\n\u001b[0;32m     79\u001b[0m \u001b[39melif\u001b[39;00m clustering_setting \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mkMeans\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\T460\\Documents\\Uni_spul\\Jaar_6\\DIS\\Project\\data_intensive_systems\\src\\clustering.py:95\u001b[0m, in \u001b[0;36mevaluate_kModes\u001b[1;34m(data, centroids, distance, perfect_centroids)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m# First, for each point, find its closest centroid and map it as (centroid, distance). Distance is calculated using NOTE scipy jaccard distance.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39m# Then, froup by centroid, compute the average distance to centroid for each group. \u001b[39;00m\n\u001b[0;32m     94\u001b[0m closest_centroids \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m point: (\u001b[39mmin\u001b[39m(centroids, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m centroid: distance(point, centroid)), distance(point, \u001b[39mmin\u001b[39m(centroids, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m centroid: distance(point, centroid)))))\n\u001b[1;32m---> 95\u001b[0m average_within_cluster_distance \u001b[39m=\u001b[39m closest_centroids\u001b[39m.\u001b[39;49mgroupByKey()\u001b[39m.\u001b[39;49mmapValues(\u001b[39mlambda\u001b[39;49;00m distances: \u001b[39msum\u001b[39;49m(distances) \u001b[39m/\u001b[39;49m \u001b[39mlen\u001b[39;49m(distances))\u001b[39m.\u001b[39;49mcollectAsMap() \u001b[39m# TODO: CHECK IF THIS IS CORRECT\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39m# Calculate distances between the centroids. TODO: Check if this should not be the average over amount of clusters. Or, if this needs a more manual method using the dstiance function\u001b[39;00m\n\u001b[0;32m     98\u001b[0m between_cluster_distance \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39mspatial\u001b[39m.\u001b[39mdistance\u001b[39m.\u001b[39mpdist(centroids, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mjaccard\u001b[39m\u001b[39m'\u001b[39m) \n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\rdd.py:3438\u001b[0m, in \u001b[0;36mRDD.collectAsMap\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollectAsMap\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[Tuple[K, V]]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[K, V]:\n\u001b[0;32m   3411\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3412\u001b[0m \u001b[39m    Return the key-value pairs in this RDD to the master as a dictionary.\u001b[39;00m\n\u001b[0;32m   3413\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3436\u001b[0m \u001b[39m    4\u001b[39;00m\n\u001b[0;32m   3437\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mdict\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect())\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Python\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 25) (145.136.150.11 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\nTypeError: 'module' object is not callable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 820, in process\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 828, in func\n    return f(iterator)\n  File \"c:\\Python\\lib\\site-packages\\pyspark\\rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\n  File \"C:\\Program Files\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\n  File \"..\\src\\clustering.py\", line 94, in <lambda>\n    closest_centroids = data.map(lambda point: (min(centroids, key=lambda centroid: distance(point, centroid)), distance(point, min(centroids, key=lambda centroid: distance(point, centroid)))))\nTypeError: 'module' object is not callable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
          ]
        }
      ],
      "source": [
        "# print(dir(clustering))\n",
        "res = clustering.jaccard_distance([1,1,0,1,0], [1,1,1,1,0])\n",
        "print(res)\n",
        "clustering.clustering_test1()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qoGwAnHNN743"
      },
      "source": [
        "**Create a spark instance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8NRmi1N743"
      },
      "outputs": [],
      "source": [
        "spark_settings = {}\n",
        "spark_instance = make_spark()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FeNSBK7WN744"
      },
      "source": [
        "**Load and parse data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg4AJxZRN744"
      },
      "outputs": [],
      "source": [
        "# Load and parse data\n",
        "data_for_spark = parse_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqUifQ0kN745"
      },
      "outputs": [],
      "source": [
        "# Run clustering and parameter tuning\n",
        "clustering_settings = {}\n",
        "clustering_centroid_outcomes = run_clustering(spark_instance, data_for_spark, clustering_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CxnhSWN745"
      },
      "outputs": [],
      "source": [
        "# Evaluate clustering results\n",
        "clustering_evaluation = evaluate_clustering(clustering_centroid_outcomes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWgrghsBN746"
      },
      "outputs": [],
      "source": [
        "# Display results (maybe only best result)\n",
        "clustering_result = display_results(clustering_evaluation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
