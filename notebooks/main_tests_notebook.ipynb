{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedorTaggenbrock/data_intensive_systems/blob/main/notebooks/main_tests_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d1m1UZvaN74z",
        "outputId": "6e57c1d0-fd72-4b3c-d553-709ee7298279",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/../main_tests.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python ../main_tests.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aPsU73HSlLI"
      },
      "source": [
        "**Pip install stuff that needs to be installed before importing anything**  \n",
        "(for colab only; cannot be done inside the modules, would be stupid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INJqht4-StzH",
        "outputId": "6f159ec4-3886-424b-8b87-1e605cffd9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=bd3670f4bd30b47807938dcc60859d6d40e0ee181448346a9a74ab877a8cde2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "ON_COLAB = 'google.colab' in sys.modules\n",
        "if ON_COLAB:\n",
        "    # Do stuff that only needs to happen on colab\n",
        "    !pip install pyspark\n",
        "else:\n",
        "    # Do stuff that only needs to happen on local computer\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcdvOolnPS8O"
      },
      "outputs": [],
      "source": [
        "if ON_COLAB:\n",
        "    print(\"Running in Colab: importing from GitHub.\")\n",
        "    # wget (\"curl\") all files from github\n",
        "    user = 'FedorTaggenbrock'\n",
        "    repo = 'data_intensive_systems'\n",
        "    src_dir = \"src\"\n",
        "    pyfiles = [\"clustering.py\"]\n",
        "\n",
        "    for pyfile in pyfiles:\n",
        "        url = f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{pyfile}\"\n",
        "        !wget --no-cache --backups=1 {url}\n",
        "\n",
        "    # Import all files\n",
        "    try:\n",
        "        import clustering as clustering\n",
        "    except Exception as e:\n",
        "        print('Error importing files on colab. Are the names right?.')\n",
        "\n",
        "else:\n",
        "    print(\"Running locally, importing from local.\")\n",
        "    # Append parent folder to path\n",
        "    sys.path.append('..')\n",
        "    # And import all files\n",
        "    from src import clustering as clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import sys\n",
        "\n",
        "def get_file_from_github(raw_url, target_file):\n",
        "    response = requests.get(raw_url)\n",
        "    response.raise_for_status()  # Ensure we got a valid response\n",
        "    with open(target_file, 'wb') as out_file:\n",
        "        out_file.write(response.content)\n",
        "\n",
        "def import_from_github(raw_urls, module_names):\n",
        "    # Set your target path here\n",
        "    # In Google Colab, you can use the current directory\n",
        "    target_path = os.getcwd()\n",
        "\n",
        "    # Download files\n",
        "    for raw_url, module_name in zip(raw_urls, module_names):\n",
        "        target_file = f\"{target_path}/{module_name}.py\"\n",
        "        if not os.path.exists(target_file):\n",
        "            get_file_from_github(raw_url, target_file)\n",
        "\n",
        "    # Add the directory to sys.path\n",
        "    sys.path.insert(0, target_path)\n",
        "\n",
        "    # Now you can import your modules\n",
        "    modules = [__import__(module_name) for module_name in module_names]\n",
        "    \n",
        "    return modules\n",
        "\n",
        "\n",
        "user = 'FedorTaggenbrock'\n",
        "repo = 'data_intensive_systems'\n",
        "src_dir = \"src\"\n",
        "module_names = [\"clustering\", 'main_tests']\n",
        "raw_urls = [f\"https://raw.githubusercontent.com/{user}/{repo}/main/{src_dir}/{modname}.py\" for modname in module_names]\n",
        "\n",
        "# github_url = 'https://github.com/FedorTaggenbrock/data_intensive_systems.git'\n",
        "\n",
        "clustering, main_tests = import_from_github(raw_urls, module_names)\n",
        "\n",
        "# Now you can use your modules\n",
        "res = clustering.jaccard_distance([1,1,0,1,0], [1,1,1,1,0])\n",
        "print(res)\n",
        "clustering.clustering_test1()"
      ],
      "metadata": {
        "id": "esFX-UWVv6QV",
        "outputId": "2af0b9c0-fee2-4ee9-ed18-ee2bb516b197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n",
            "update 13:30\n",
            "Initialized Spark. Start clustering.\n",
            "centroids =  [(1, 0, 0, 1, 0), (1, 1, 1, 1, 0)]\n",
            "centroids =  [(1, 0, 0, 1, 0), (1, 1, 1, 1, 0)]\n",
            "Finished clustering. Start evaluation.\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_cMlVTtz4Ad",
        "outputId": "3f4b8975-c1f8-4c2c-f6ed-c2de70706ee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ArrayType',\n",
              " 'List',\n",
              " 'RDD',\n",
              " 'SparkContext',\n",
              " 'SparkSession',\n",
              " 'StringType',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__spec__',\n",
              " 'clustering_test1',\n",
              " 'evaluate_clustering',\n",
              " 'evaluate_kModes',\n",
              " 'jaccard_distance',\n",
              " 'kModes_v2',\n",
              " 'mode',\n",
              " 'np',\n",
              " 'scipy',\n",
              " 'udf']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wxgQD6vN741"
      },
      "source": [
        "**Test if this works**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEkh8sL6N742",
        "outputId": "d53c4e9c-b011-44d2-bd09-1622cf0c6307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n",
            "Initialized Spark. Start clustering.\n",
            "centroids =  [(1, 0, 0, 1, 0), (1, 0, 0, 0, 1)]\n",
            "centroids =  [(1, 1, 0, 1, 0), (0, 0, 1, 0, 1)]\n",
            "Finished clustering. Start evaluation.\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# print(dir(clustering))\n",
        "res = clustering.jaccard_distance([1,1,0,1,0], [1,1,1,1,0])\n",
        "print(res)\n",
        "clustering.clustering_test1()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoGwAnHNN743"
      },
      "source": [
        "**Create a spark instance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN8NRmi1N743"
      },
      "outputs": [],
      "source": [
        "spark_settings = {}\n",
        "spark_instance = make_spark()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeNSBK7WN744"
      },
      "source": [
        "**Load and parse data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg4AJxZRN744"
      },
      "outputs": [],
      "source": [
        "# Load and parse data\n",
        "data_for_spark = parse_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqUifQ0kN745"
      },
      "outputs": [],
      "source": [
        "# Run clustering and parameter tuning\n",
        "clustering_settings = {}\n",
        "clustering_centroid_outcomes = run_clustering(spark_instance, data_for_spark, clustering_settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06CxnhSWN745"
      },
      "outputs": [],
      "source": [
        "# Evaluate clustering results\n",
        "clustering_evaluation = evaluate_clustering(clustering_centroid_outcomes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWgrghsBN746"
      },
      "outputs": [],
      "source": [
        "# Display results (maybe only best result)\n",
        "clustering_result = display_results(clustering_evaluation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}